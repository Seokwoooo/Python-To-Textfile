import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel
import numpy as np
import os
import sys
import re
import json
import argparse
import json
import traceback

# model.py에서 필요한 클래스와 함수를 가져옵니다
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from model import SoomgoServiceClassifier, get_device, load_model

# 상수 정의
MAX_LENGTH = 512  # 토크나이저 최대 길이
CONFIDENCE_THRESHOLD = 0.7  # 적합성 판단을 위한 신뢰도 임계값
MODEL_PATH = "AI_RPA/unified_output/model.pt"  # 모델 파일 경로
TOKENIZER_PATH = "AI_RPA/unified_output/tokenizer"  # 토크나이저 경로

def format_input_text(text):
    """
    입력 텍스트에 자동으로 줄바꿈을 추가하는 함수.
    '[Q]', '[A]', '고객:', '고수:' 앞에 줄바꿈을 추가합니다.
    (단, 문자열 시작 부분 제외)
    
    Args:
        text (str): 입력 텍스트
        
    Returns:
        str: 줄바꿈이 추가된 텍스트
    """
    if not text:
        return ""
    # [Q], [A] 앞에 줄바꿈 추가 (이미 줄바꿈된 경우는 제외)
    text = re.sub(r'(?<!^)\s*(\[[QA]\])', r'\n\1', text)
    # 고객:, 고수: 앞에 줄바꿈 추가 (이미 줄바꿈된 경우는 제외)
    text = re.sub(r'(?<!^)\s*(고[객수]:)', r'\n\1', text)
    return text.strip()

def preprocess_request_text(text):
    """
    요청서 텍스트 전처리 함수
    
    Args:
        text (str): 전처리할 텍스트
        
    Returns:
        str: 전처리된 텍스트
    """
    if not text:
        return ""
        
    # Q&A 구조에서 질문과 답변 모두 추출
    qa_pairs = []
    lines = text.split('\n')
    for i in range(len(lines)):
        line = lines[i]
        if '[Q]' in line and i + 1 < len(lines) and '[A]' in lines[i + 1]:
            question = line.split('[Q]')[1].strip()
            answer = lines[i + 1].split('[A]')[1].strip()
            if answer and answer != '고수와 상담시 논의할게요':
                qa_pairs.append(f"질문: {question} 답변: {answer}")
    
    # 질문-답변 쌍들을 하나의 텍스트로 결합 (없으면 원본 텍스트 사용)
    processed_text = ' '.join(qa_pairs) if qa_pairs else text
    
    # 특수문자 제거 (단, 기본 구두점은 유지)
    processed_text = re.sub(r'[^\w\s\.,!?]', ' ', processed_text)
    # 연속된 공백 제거
    processed_text = re.sub(r'\s+', ' ', processed_text)
    # 앞뒤 공백 제거
    processed_text = processed_text.strip()
    
    return processed_text

def preprocess_chat_text(text):
    """
    채팅 텍스트 전처리 함수
    
    Args:
        text (str): 전처리할 텍스트
        
    Returns:
        str: 전처리된 텍스트
    """
    if not text:
        return ""
    
    # 채팅 내용에서 유용한 정보 추출
    chat_lines = []
    for line in text.split('\n'):
        # 고객/고수 발화 내용 추출
        if '고객:' in line or '고수:' in line:
            parts = line.split(':', 1)
            if len(parts) == 2:
                speaker = parts[0].strip()
                content = parts[1].strip()
                if content and len(content) > 1:  # 짧은 응답 필터링
                    chat_lines.append(f"{speaker}: {content}")
        # 시간 정보가 있는 일반 채팅 내용 추출
        elif ':' in line:
            parts = line.split(':', 1)
            if len(parts) == 2 and not parts[0].strip().isdigit():  # 숫자만 있는 경우는 시간일 가능성이 높으므로 제외
                content = parts[1].strip()
                if content and len(content) > 1:  # 짧은 응답 필터링
                    chat_lines.append(content)
    
    # 채팅 내용 결합
    processed_text = ' '.join(chat_lines) if chat_lines else text
    
    # 특수문자 제거 (단, 기본 구두점은 유지)
    processed_text = re.sub(r'[^\w\s\.,!?]', ' ', processed_text)
    # 연속된 공백 제거
    processed_text = re.sub(r'\s+', ' ', processed_text)
    # 앞뒤 공백 제거
    processed_text = processed_text.strip()
    
    return processed_text

def tokenize_text(text, tokenizer, max_length=512):
    """
    텍스트를 토큰화하는 함수
    
    Args:
        text (str): 토큰화할 텍스트
        tokenizer: 사용할 토크나이저
        max_length (int): 최대 시퀀스 길이
        
    Returns:
        dict: 토큰화된 입력
    """
    return tokenizer(
        text,
        padding='max_length',
        truncation=True,
        max_length=max_length,
        return_tensors='pt'
    )

def predict_request(request_text, chat_text, model, tokenizer, device):
    """
    요청 텍스트와 채팅 텍스트로 서비스 적합성과 유형을 예측하는 함수
    
    Args:
        request_text (str): 요청 텍스트
        chat_text (str): 채팅 텍스트
        model: 학습된 모델
        tokenizer: 토크나이저
        device: 사용할 디바이스
        
    Returns:
        dict: 예측 결과 딕셔너리
    """
    # 텍스트 전처리
    processed_request = preprocess_request_text(request_text)
    processed_chat = preprocess_chat_text(chat_text)
    
    # 텍스트 토큰화
    request_encoded = tokenize_text(processed_request, tokenizer)
    
    # 채팅 텍스트가 있으면 토큰화, 없으면 빈 문자열로 토큰화
    if not processed_chat:
        processed_chat = ""
    chat_encoded = tokenize_text(processed_chat, tokenizer)
    
    # 토큰화된 입력을 디바이스로 이동
    request_input_ids = request_encoded['input_ids'].to(device)
    request_attention_mask = request_encoded['attention_mask'].to(device)
    chat_input_ids = chat_encoded['input_ids'].to(device)
    chat_attention_mask = chat_encoded['attention_mask'].to(device)
    
    # 예측 수행
    model.eval()
    with torch.no_grad():
        logits, probs, service_logits = model(
            request_input_ids, 
            request_attention_mask, 
            chat_input_ids, 
            chat_attention_mask
        )
        
        # 예측 결과 및 확률 계산
        prediction = torch.argmax(probs, dim=1).cpu().numpy()[0]
        confidence = probs[0][prediction].cpu().numpy()
        service_type = torch.argmax(service_logits, dim=1).cpu().numpy()[0]
        
    # 서비스 유형 매핑
    service_mapping = {0: "입주청소", 1: "이사청소", 2: "거주청소", 3: "기타청소"}
    
    # 결과 반환
    result = {
        "is_suitable": "적합" if prediction == 0 else "부적합",
        "confidence": float(confidence),
        "service_type": service_mapping.get(service_type, "알 수 없음")
    }
    
    return result

def predict_suitability_improved(user_input, service_desc, model, tokenizer):
    """
    개선된 서비스 적합성 예측 함수입니다.
    
    Args:
        user_input (str): 사용자 입력 텍스트
        service_desc (str): 서비스 설명 텍스트
        model: 학습된 모델
        tokenizer: 토크나이저
        
    Returns:
        dict: 예측 결과와 설명을 포함한 딕셔너리
    """
    # 1. Q-A 쌍 추출 및 일관성 분석
    user_qa_pairs = extract_qa_pairs(user_input)
    service_qa_pairs = extract_qa_pairs(service_desc)
    
    user_consistency = analyze_qa_consistency(user_qa_pairs, model, tokenizer)
    service_consistency = analyze_qa_consistency(service_qa_pairs, model, tokenizer)
    
    # 2. 서비스 관련성 분석
    relevance_score = analyze_service_relevance(user_input, service_desc, model, tokenizer)
    
    # 3. 종합 점수 계산
    # 일관성과 관련성을 가중 평균하여 최종 점수 계산
    final_score = (0.3 * user_consistency + 
                  0.3 * service_consistency + 
                  0.4 * relevance_score)
    
    # 4. 결과 해석 및 설명 생성
    is_suitable = final_score >= CONFIDENCE_THRESHOLD
    explanation = {
        "user_input_consistency": f"사용자 입력의 내부 일관성: {user_consistency:.2f}",
        "service_desc_consistency": f"서비스 설명의 내부 일관성: {service_consistency:.2f}",
        "relevance_score": f"서비스 관련성 점수: {relevance_score:.2f}",
        "final_score": f"종합 점수: {final_score:.2f}",
        "threshold": f"판단 기준값: {CONFIDENCE_THRESHOLD}",
        "qa_pairs": {
            "user_input": [{"Q": q, "A": a} for q, a in user_qa_pairs],
            "service_desc": [{"Q": q, "A": a} for q, a in service_qa_pairs]
        }
    }
    
    return {
        "is_suitable": is_suitable,
        "confidence_score": final_score,
        "explanation": explanation
    }

def extract_qa_pairs(text):
    """
    텍스트에서 Q-A 쌍을 추출합니다.
    
    Args:
        text (str): 분석할 텍스트
        
    Returns:
        list: (질문, 답변) 튜플의 리스트
    """
    qa_pairs = []
    lines = text.split('\n')
    
    for i in range(len(lines)-1):
        current_line = lines[i].strip()
        next_line = lines[i+1].strip()
        
        # Q로 시작하는 줄을 질문으로, A로 시작하는 다음 줄을 답변으로 간주
        if current_line.startswith('Q:') and next_line.startswith('A:'):
            question = current_line[2:].strip()
            answer = next_line[2:].strip()
            qa_pairs.append((question, answer))
            
    return qa_pairs

def analyze_qa_consistency(qa_pairs, model, tokenizer):
    """
    Q-A 쌍들의 내부 일관성을 분석합니다.
    
    Args:
        qa_pairs (list): (질문, 답변) 튜플의 리스트
        model: 학습된 모델
        tokenizer: 토크나이저
        
    Returns:
        float: 일관성 점수 (0~1)
    """
    if not qa_pairs:
        return 0.0
        
    consistency_scores = []
    
    for question, answer in qa_pairs:
        # Q-A 쌍을 하나의 시퀀스로 결합
        combined = f"{question} [SEP] {answer}"
        inputs = tokenizer(combined, 
                         return_tensors="pt",
                         max_length=MAX_LENGTH,
                         truncation=True,
                         padding=True)
        
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            probs = F.softmax(logits, dim=-1)
            consistency_score = probs[0][1].item()  # positive class probability
            consistency_scores.append(consistency_score)
    
    # 모든 Q-A 쌍의 평균 일관성 점수 반환
    return sum(consistency_scores) / len(consistency_scores)

def analyze_service_relevance(user_input, service_desc, model, tokenizer):
    """
    사용자 입력과 서비스 설명 간의 관련성을 분석합니다.
    
    Args:
        user_input (str): 사용자 입력 텍스트
        service_desc (str): 서비스 설명 텍스트
        model: 학습된 모델
        tokenizer: 토크나이저
        
    Returns:
        float: 관련성 점수 (0~1)
    """
    # 입력을 하나의 시퀀스로 결합
    combined = f"{user_input} [SEP] {service_desc}"
    inputs = tokenizer(combined, 
                      return_tensors="pt",
                      max_length=MAX_LENGTH,
                      truncation=True,
                      padding=True)
    
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = F.softmax(logits, dim=-1)
        relevance_score = probs[0][1].item()  # positive class probability
        
    return relevance_score

def load_service_descriptions():
    """
    서비스 설명 데이터를 로드하는 함수입니다.
    
    Returns:
        dict: 서비스 이름을 키로 하고 서비스 설명을 값으로 하는 딕셔너리
    """
    # 기본 서비스 설명 데이터
    return {
        "이사/입주청소": """
            Q: 어떤 종류의 청소 서비스를 제공하나요?
            A: 이사 및 입주 시 필요한 전문 청소 서비스를 제공합니다.
            
            Q: 어떤 공간을 청소할 수 있나요?
            A: 아파트, 빌라, 주택, 원룸, 오피스텔 등 모든 주거공간과 상업공간을 청소할 수 있습니다.
            
            Q: 어떤 청소 항목이 포함되나요?
            A: 방, 거실, 주방, 화장실, 베란다 등 모든 생활공간의 청소가 포함됩니다. 특히 찌든 때, 물때, 기름때 제거에 특화되어 있습니다.
            
            Q: 청소 시간은 얼마나 걸리나요?
            A: 공간 크기와 상태에 따라 다르지만, 보통 4-8시간 정도 소요됩니다.
        """,
        
        "정기청소": """
            Q: 정기청소는 어떤 주기로 진행되나요?
            A: 주 1회, 2주 1회, 월 1회 등 고객이 원하는 주기로 맞춤 서비스를 제공합니다.
            
            Q: 어떤 청소 항목이 포함되나요?
            A: 일상적인 청소(청소기, 물걸레질), 화장실 청소, 주방 청소, 정리정돈이 기본으로 포함됩니다.
            
            Q: 청소 시간은 얼마나 걸리나요?
            A: 일반적으로 2-3시간 정도 소요되며, 공간 크기와 상태에 따라 조정 가능합니다.
        """,
        
        "특수청소": """
            Q: 특수청소는 어떤 서비스인가요?
            A: 곰팡이 제거, 물때 제거, 기름때 제거, 스티커 제거 등 특수한 청소 기술이 필요한 서비스입니다.
            
            Q: 어떤 도구와 약품을 사용하나요?
            A: 전문 청소 도구와 친환경 세제를 사용하여 안전하고 효과적인 청소를 진행합니다.
            
            Q: 보증은 어떻게 되나요?
            A: 청소 후 문제 발생 시 무상 A/S를 제공합니다.
        """
    }

def main():
    """
    메인 실행 함수입니다.
    사용자 입력을 받아 서비스 적합도를 예측하고 결과를 출력합니다.
    """
    try:
        # 모델과 토크나이저 로드
        print("모델과 토크나이저를 로드하는 중...")
        model = torch.load("AI_RPA/unified_output/model.pt")
        tokenizer = AutoTokenizer.from_pretrained("klue/roberta-large")
        model.eval()
        
        # 서비스 설명 로드
        service_descriptions = load_service_descriptions()
        
        while True:
            # 사용자 입력 받기
            print("\n=== 서비스 적합도 분석 시스템 ===")
            print("종료하려면 'q' 또는 'quit'를 입력하세요.")
            user_input = input("\n고객님의 요구사항을 입력해주세요: ")
            
            if user_input.lower() in ['q', 'quit']:
                print("프로그램을 종료합니다.")
                break
            
            print("\n분석 중...")
            results = []
            
            # 각 서비스에 대해 적합도 분석
            for service_name, service_desc in service_descriptions.items():
                prediction = predict_suitability_improved(
                    user_input=user_input,
                    service_desc=service_desc,
                    model=model,
                    tokenizer=tokenizer
                )
                
                results.append({
                    "service_name": service_name,
                    "prediction": prediction
                })
            
            # 결과 정렬 (점수 기준 내림차순)
            results.sort(key=lambda x: x["prediction"]["confidence_score"], reverse=True)
            
            # 결과 출력
            print("\n=== 분석 결과 ===")
            for result in results:
                service_name = result["service_name"]
                pred = result["prediction"]
                
                print(f"\n서비스: {service_name}")
                print(f"최종 점수: {pred['confidence_score']:.2f}")
                
                if pred['confidence_score'] >= 0.7:
                    print("\n✅ 추천: 이 서비스는 고객님의 요구사항과 매우 잘 맞습니다.")
                elif pred['confidence_score'] >= 0.5:
                    print("\n🤔 참고: 이 서비스는 부분적으로 요구사항을 충족할 수 있습니다.")
                else:
                    print("\n❌ 주의: 이 서비스는 고객님의 요구사항과 맞지 않을 수 있습니다.")
                
                print("\n분석 내용:")
                for key, value in pred["explanation"].items():
                    if key != "qa_pairs":
                        print(f"- {value}")
            
            print("\n분석이 완료되었습니다.")
            
    except Exception as e:
        print(f"오류 발생: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main() 