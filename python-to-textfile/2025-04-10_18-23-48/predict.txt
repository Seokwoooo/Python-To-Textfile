import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel
import numpy as np
import os
import sys
import re
import json
import argparse
import json
import traceback

# model.pyì—ì„œ í•„ìš”í•œ í´ë˜ìŠ¤ì™€ í•¨ìˆ˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from model import SoomgoServiceClassifier, get_device, load_model

# ìƒìˆ˜ ì •ì˜
MAX_LENGTH = 512  # í† í¬ë‚˜ì´ì € ìµœëŒ€ ê¸¸ì´
CONFIDENCE_THRESHOLD = 0.7  # ì í•©ì„± íŒë‹¨ì„ ìœ„í•œ ì‹ ë¢°ë„ ì„ê³„ê°’
MODEL_PATH = "AI_RPA/unified_output/model.pt"  # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
TOKENIZER_PATH = "AI_RPA/unified_output/tokenizer"  # í† í¬ë‚˜ì´ì € ê²½ë¡œ

def format_input_text(text):
    """
    ì…ë ¥ í…ìŠ¤íŠ¸ì— ìë™ìœ¼ë¡œ ì¤„ë°”ê¿ˆì„ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜.
    '[Q]', '[A]', 'ê³ ê°:', 'ê³ ìˆ˜:' ì•ì— ì¤„ë°”ê¿ˆì„ ì¶”ê°€í•©ë‹ˆë‹¤.
    (ë‹¨, ë¬¸ìì—´ ì‹œì‘ ë¶€ë¶„ ì œì™¸)
    
    Args:
        text (str): ì…ë ¥ í…ìŠ¤íŠ¸
        
    Returns:
        str: ì¤„ë°”ê¿ˆì´ ì¶”ê°€ëœ í…ìŠ¤íŠ¸
    """
    if not text:
        return ""
    # [Q], [A] ì•ì— ì¤„ë°”ê¿ˆ ì¶”ê°€ (ì´ë¯¸ ì¤„ë°”ê¿ˆëœ ê²½ìš°ëŠ” ì œì™¸)
    text = re.sub(r'(?<!^)\s*(\[[QA]\])', r'\n\1', text)
    # ê³ ê°:, ê³ ìˆ˜: ì•ì— ì¤„ë°”ê¿ˆ ì¶”ê°€ (ì´ë¯¸ ì¤„ë°”ê¿ˆëœ ê²½ìš°ëŠ” ì œì™¸)
    text = re.sub(r'(?<!^)\s*(ê³ [ê°ìˆ˜]:)', r'\n\1', text)
    return text.strip()

def preprocess_request_text(text):
    """
    ìš”ì²­ì„œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
    
    Args:
        text (str): ì „ì²˜ë¦¬í•  í…ìŠ¤íŠ¸
        
    Returns:
        str: ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
    """
    if not text:
        return ""
        
    # Q&A êµ¬ì¡°ì—ì„œ ì§ˆë¬¸ê³¼ ë‹µë³€ ëª¨ë‘ ì¶”ì¶œ
    qa_pairs = []
    lines = text.split('\n')
    for i in range(len(lines)):
        line = lines[i]
        if '[Q]' in line and i + 1 < len(lines) and '[A]' in lines[i + 1]:
            question = line.split('[Q]')[1].strip()
            answer = lines[i + 1].split('[A]')[1].strip()
            if answer and answer != 'ê³ ìˆ˜ì™€ ìƒë‹´ì‹œ ë…¼ì˜í• ê²Œìš”':
                qa_pairs.append(f"ì§ˆë¬¸: {question} ë‹µë³€: {answer}")
    
    # ì§ˆë¬¸-ë‹µë³€ ìŒë“¤ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•© (ì—†ìœ¼ë©´ ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©)
    processed_text = ' '.join(qa_pairs) if qa_pairs else text
    
    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ë‹¨, ê¸°ë³¸ êµ¬ë‘ì ì€ ìœ ì§€)
    processed_text = re.sub(r'[^\w\s\.,!?]', ' ', processed_text)
    # ì—°ì†ëœ ê³µë°± ì œê±°
    processed_text = re.sub(r'\s+', ' ', processed_text)
    # ì•ë’¤ ê³µë°± ì œê±°
    processed_text = processed_text.strip()
    
    return processed_text

def preprocess_chat_text(text):
    """
    ì±„íŒ… í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
    
    Args:
        text (str): ì „ì²˜ë¦¬í•  í…ìŠ¤íŠ¸
        
    Returns:
        str: ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
    """
    if not text:
        return ""
    
    # ì±„íŒ… ë‚´ìš©ì—ì„œ ìœ ìš©í•œ ì •ë³´ ì¶”ì¶œ
    chat_lines = []
    for line in text.split('\n'):
        # ê³ ê°/ê³ ìˆ˜ ë°œí™” ë‚´ìš© ì¶”ì¶œ
        if 'ê³ ê°:' in line or 'ê³ ìˆ˜:' in line:
            parts = line.split(':', 1)
            if len(parts) == 2:
                speaker = parts[0].strip()
                content = parts[1].strip()
                if content and len(content) > 1:  # ì§§ì€ ì‘ë‹µ í•„í„°ë§
                    chat_lines.append(f"{speaker}: {content}")
        # ì‹œê°„ ì •ë³´ê°€ ìˆëŠ” ì¼ë°˜ ì±„íŒ… ë‚´ìš© ì¶”ì¶œ
        elif ':' in line:
            parts = line.split(':', 1)
            if len(parts) == 2 and not parts[0].strip().isdigit():  # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš°ëŠ” ì‹œê°„ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ ì œì™¸
                content = parts[1].strip()
                if content and len(content) > 1:  # ì§§ì€ ì‘ë‹µ í•„í„°ë§
                    chat_lines.append(content)
    
    # ì±„íŒ… ë‚´ìš© ê²°í•©
    processed_text = ' '.join(chat_lines) if chat_lines else text
    
    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ë‹¨, ê¸°ë³¸ êµ¬ë‘ì ì€ ìœ ì§€)
    processed_text = re.sub(r'[^\w\s\.,!?]', ' ', processed_text)
    # ì—°ì†ëœ ê³µë°± ì œê±°
    processed_text = re.sub(r'\s+', ' ', processed_text)
    # ì•ë’¤ ê³µë°± ì œê±°
    processed_text = processed_text.strip()
    
    return processed_text

def tokenize_text(text, tokenizer, max_length=512):
    """
    í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        text (str): í† í°í™”í•  í…ìŠ¤íŠ¸
        tokenizer: ì‚¬ìš©í•  í† í¬ë‚˜ì´ì €
        max_length (int): ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        
    Returns:
        dict: í† í°í™”ëœ ì…ë ¥
    """
    return tokenizer(
        text,
        padding='max_length',
        truncation=True,
        max_length=max_length,
        return_tensors='pt'
    )

def predict_request(request_text, chat_text, model, tokenizer, device):
    """
    ìš”ì²­ í…ìŠ¤íŠ¸ì™€ ì±„íŒ… í…ìŠ¤íŠ¸ë¡œ ì„œë¹„ìŠ¤ ì í•©ì„±ê³¼ ìœ í˜•ì„ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        request_text (str): ìš”ì²­ í…ìŠ¤íŠ¸
        chat_text (str): ì±„íŒ… í…ìŠ¤íŠ¸
        model: í•™ìŠµëœ ëª¨ë¸
        tokenizer: í† í¬ë‚˜ì´ì €
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        
    Returns:
        dict: ì˜ˆì¸¡ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    processed_request = preprocess_request_text(request_text)
    processed_chat = preprocess_chat_text(chat_text)
    
    # í…ìŠ¤íŠ¸ í† í°í™”
    request_encoded = tokenize_text(processed_request, tokenizer)
    
    # ì±„íŒ… í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ í† í°í™”, ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ í† í°í™”
    if not processed_chat:
        processed_chat = ""
    chat_encoded = tokenize_text(processed_chat, tokenizer)
    
    # í† í°í™”ëœ ì…ë ¥ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    request_input_ids = request_encoded['input_ids'].to(device)
    request_attention_mask = request_encoded['attention_mask'].to(device)
    chat_input_ids = chat_encoded['input_ids'].to(device)
    chat_attention_mask = chat_encoded['attention_mask'].to(device)
    
    # ì˜ˆì¸¡ ìˆ˜í–‰
    model.eval()
    with torch.no_grad():
        logits, probs, service_logits = model(
            request_input_ids, 
            request_attention_mask, 
            chat_input_ids, 
            chat_attention_mask
        )
        
        # ì˜ˆì¸¡ ê²°ê³¼ ë° í™•ë¥  ê³„ì‚°
        prediction = torch.argmax(probs, dim=1).cpu().numpy()[0]
        confidence = probs[0][prediction].cpu().numpy()
        service_type = torch.argmax(service_logits, dim=1).cpu().numpy()[0]
        
    # ì„œë¹„ìŠ¤ ìœ í˜• ë§¤í•‘
    service_mapping = {0: "ì…ì£¼ì²­ì†Œ", 1: "ì´ì‚¬ì²­ì†Œ", 2: "ê±°ì£¼ì²­ì†Œ", 3: "ê¸°íƒ€ì²­ì†Œ"}
    
    # ê²°ê³¼ ë°˜í™˜
    result = {
        "is_suitable": "ì í•©" if prediction == 0 else "ë¶€ì í•©",
        "confidence": float(confidence),
        "service_type": service_mapping.get(service_type, "ì•Œ ìˆ˜ ì—†ìŒ")
    }
    
    return result

def predict_suitability_improved(user_input, service_desc, model, tokenizer):
    """
    ê°œì„ ëœ ì„œë¹„ìŠ¤ ì í•©ì„± ì˜ˆì¸¡ í•¨ìˆ˜ì…ë‹ˆë‹¤.
    
    Args:
        user_input (str): ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
        service_desc (str): ì„œë¹„ìŠ¤ ì„¤ëª… í…ìŠ¤íŠ¸
        model: í•™ìŠµëœ ëª¨ë¸
        tokenizer: í† í¬ë‚˜ì´ì €
        
    Returns:
        dict: ì˜ˆì¸¡ ê²°ê³¼ì™€ ì„¤ëª…ì„ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
    """
    # 1. Q-A ìŒ ì¶”ì¶œ ë° ì¼ê´€ì„± ë¶„ì„
    user_qa_pairs = extract_qa_pairs(user_input)
    service_qa_pairs = extract_qa_pairs(service_desc)
    
    user_consistency = analyze_qa_consistency(user_qa_pairs, model, tokenizer)
    service_consistency = analyze_qa_consistency(service_qa_pairs, model, tokenizer)
    
    # 2. ì„œë¹„ìŠ¤ ê´€ë ¨ì„± ë¶„ì„
    relevance_score = analyze_service_relevance(user_input, service_desc, model, tokenizer)
    
    # 3. ì¢…í•© ì ìˆ˜ ê³„ì‚°
    # ì¼ê´€ì„±ê³¼ ê´€ë ¨ì„±ì„ ê°€ì¤‘ í‰ê· í•˜ì—¬ ìµœì¢… ì ìˆ˜ ê³„ì‚°
    final_score = (0.3 * user_consistency + 
                  0.3 * service_consistency + 
                  0.4 * relevance_score)
    
    # 4. ê²°ê³¼ í•´ì„ ë° ì„¤ëª… ìƒì„±
    is_suitable = final_score >= CONFIDENCE_THRESHOLD
    explanation = {
        "user_input_consistency": f"ì‚¬ìš©ì ì…ë ¥ì˜ ë‚´ë¶€ ì¼ê´€ì„±: {user_consistency:.2f}",
        "service_desc_consistency": f"ì„œë¹„ìŠ¤ ì„¤ëª…ì˜ ë‚´ë¶€ ì¼ê´€ì„±: {service_consistency:.2f}",
        "relevance_score": f"ì„œë¹„ìŠ¤ ê´€ë ¨ì„± ì ìˆ˜: {relevance_score:.2f}",
        "final_score": f"ì¢…í•© ì ìˆ˜: {final_score:.2f}",
        "threshold": f"íŒë‹¨ ê¸°ì¤€ê°’: {CONFIDENCE_THRESHOLD}",
        "qa_pairs": {
            "user_input": [{"Q": q, "A": a} for q, a in user_qa_pairs],
            "service_desc": [{"Q": q, "A": a} for q, a in service_qa_pairs]
        }
    }
    
    return {
        "is_suitable": is_suitable,
        "confidence_score": final_score,
        "explanation": explanation
    }

def extract_qa_pairs(text):
    """
    í…ìŠ¤íŠ¸ì—ì„œ Q-A ìŒì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸
        
    Returns:
        list: (ì§ˆë¬¸, ë‹µë³€) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸
    """
    qa_pairs = []
    lines = text.split('\n')
    
    for i in range(len(lines)-1):
        current_line = lines[i].strip()
        next_line = lines[i+1].strip()
        
        # Që¡œ ì‹œì‘í•˜ëŠ” ì¤„ì„ ì§ˆë¬¸ìœ¼ë¡œ, Aë¡œ ì‹œì‘í•˜ëŠ” ë‹¤ìŒ ì¤„ì„ ë‹µë³€ìœ¼ë¡œ ê°„ì£¼
        if current_line.startswith('Q:') and next_line.startswith('A:'):
            question = current_line[2:].strip()
            answer = next_line[2:].strip()
            qa_pairs.append((question, answer))
            
    return qa_pairs

def analyze_qa_consistency(qa_pairs, model, tokenizer):
    """
    Q-A ìŒë“¤ì˜ ë‚´ë¶€ ì¼ê´€ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        qa_pairs (list): (ì§ˆë¬¸, ë‹µë³€) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸
        model: í•™ìŠµëœ ëª¨ë¸
        tokenizer: í† í¬ë‚˜ì´ì €
        
    Returns:
        float: ì¼ê´€ì„± ì ìˆ˜ (0~1)
    """
    if not qa_pairs:
        return 0.0
        
    consistency_scores = []
    
    for question, answer in qa_pairs:
        # Q-A ìŒì„ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë¡œ ê²°í•©
        combined = f"{question} [SEP] {answer}"
        inputs = tokenizer(combined, 
                         return_tensors="pt",
                         max_length=MAX_LENGTH,
                         truncation=True,
                         padding=True)
        
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            probs = F.softmax(logits, dim=-1)
            consistency_score = probs[0][1].item()  # positive class probability
            consistency_scores.append(consistency_score)
    
    # ëª¨ë“  Q-A ìŒì˜ í‰ê·  ì¼ê´€ì„± ì ìˆ˜ ë°˜í™˜
    return sum(consistency_scores) / len(consistency_scores)

def analyze_service_relevance(user_input, service_desc, model, tokenizer):
    """
    ì‚¬ìš©ì ì…ë ¥ê³¼ ì„œë¹„ìŠ¤ ì„¤ëª… ê°„ì˜ ê´€ë ¨ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        user_input (str): ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
        service_desc (str): ì„œë¹„ìŠ¤ ì„¤ëª… í…ìŠ¤íŠ¸
        model: í•™ìŠµëœ ëª¨ë¸
        tokenizer: í† í¬ë‚˜ì´ì €
        
    Returns:
        float: ê´€ë ¨ì„± ì ìˆ˜ (0~1)
    """
    # ì…ë ¥ì„ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë¡œ ê²°í•©
    combined = f"{user_input} [SEP] {service_desc}"
    inputs = tokenizer(combined, 
                      return_tensors="pt",
                      max_length=MAX_LENGTH,
                      truncation=True,
                      padding=True)
    
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = F.softmax(logits, dim=-1)
        relevance_score = probs[0][1].item()  # positive class probability
        
    return relevance_score

def load_service_descriptions():
    """
    ì„œë¹„ìŠ¤ ì„¤ëª… ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    
    Returns:
        dict: ì„œë¹„ìŠ¤ ì´ë¦„ì„ í‚¤ë¡œ í•˜ê³  ì„œë¹„ìŠ¤ ì„¤ëª…ì„ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
    """
    # ê¸°ë³¸ ì„œë¹„ìŠ¤ ì„¤ëª… ë°ì´í„°
    return {
        "ì´ì‚¬/ì…ì£¼ì²­ì†Œ": """
            Q: ì–´ë–¤ ì¢…ë¥˜ì˜ ì²­ì†Œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ë‚˜ìš”?
            A: ì´ì‚¬ ë° ì…ì£¼ ì‹œ í•„ìš”í•œ ì „ë¬¸ ì²­ì†Œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
            
            Q: ì–´ë–¤ ê³µê°„ì„ ì²­ì†Œí•  ìˆ˜ ìˆë‚˜ìš”?
            A: ì•„íŒŒíŠ¸, ë¹Œë¼, ì£¼íƒ, ì›ë£¸, ì˜¤í”¼ìŠ¤í…” ë“± ëª¨ë“  ì£¼ê±°ê³µê°„ê³¼ ìƒì—…ê³µê°„ì„ ì²­ì†Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            
            Q: ì–´ë–¤ ì²­ì†Œ í•­ëª©ì´ í¬í•¨ë˜ë‚˜ìš”?
            A: ë°©, ê±°ì‹¤, ì£¼ë°©, í™”ì¥ì‹¤, ë² ë€ë‹¤ ë“± ëª¨ë“  ìƒí™œê³µê°„ì˜ ì²­ì†Œê°€ í¬í•¨ë©ë‹ˆë‹¤. íŠ¹íˆ ì°Œë“  ë•Œ, ë¬¼ë•Œ, ê¸°ë¦„ë•Œ ì œê±°ì— íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
            
            Q: ì²­ì†Œ ì‹œê°„ì€ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?
            A: ê³µê°„ í¬ê¸°ì™€ ìƒíƒœì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ, ë³´í†µ 4-8ì‹œê°„ ì •ë„ ì†Œìš”ë©ë‹ˆë‹¤.
        """,
        
        "ì •ê¸°ì²­ì†Œ": """
            Q: ì •ê¸°ì²­ì†ŒëŠ” ì–´ë–¤ ì£¼ê¸°ë¡œ ì§„í–‰ë˜ë‚˜ìš”?
            A: ì£¼ 1íšŒ, 2ì£¼ 1íšŒ, ì›” 1íšŒ ë“± ê³ ê°ì´ ì›í•˜ëŠ” ì£¼ê¸°ë¡œ ë§ì¶¤ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
            
            Q: ì–´ë–¤ ì²­ì†Œ í•­ëª©ì´ í¬í•¨ë˜ë‚˜ìš”?
            A: ì¼ìƒì ì¸ ì²­ì†Œ(ì²­ì†Œê¸°, ë¬¼ê±¸ë ˆì§ˆ), í™”ì¥ì‹¤ ì²­ì†Œ, ì£¼ë°© ì²­ì†Œ, ì •ë¦¬ì •ëˆì´ ê¸°ë³¸ìœ¼ë¡œ í¬í•¨ë©ë‹ˆë‹¤.
            
            Q: ì²­ì†Œ ì‹œê°„ì€ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?
            A: ì¼ë°˜ì ìœ¼ë¡œ 2-3ì‹œê°„ ì •ë„ ì†Œìš”ë˜ë©°, ê³µê°„ í¬ê¸°ì™€ ìƒíƒœì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥í•©ë‹ˆë‹¤.
        """,
        
        "íŠ¹ìˆ˜ì²­ì†Œ": """
            Q: íŠ¹ìˆ˜ì²­ì†ŒëŠ” ì–´ë–¤ ì„œë¹„ìŠ¤ì¸ê°€ìš”?
            A: ê³°íŒ¡ì´ ì œê±°, ë¬¼ë•Œ ì œê±°, ê¸°ë¦„ë•Œ ì œê±°, ìŠ¤í‹°ì»¤ ì œê±° ë“± íŠ¹ìˆ˜í•œ ì²­ì†Œ ê¸°ìˆ ì´ í•„ìš”í•œ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.
            
            Q: ì–´ë–¤ ë„êµ¬ì™€ ì•½í’ˆì„ ì‚¬ìš©í•˜ë‚˜ìš”?
            A: ì „ë¬¸ ì²­ì†Œ ë„êµ¬ì™€ ì¹œí™˜ê²½ ì„¸ì œë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê³  íš¨ê³¼ì ì¸ ì²­ì†Œë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.
            
            Q: ë³´ì¦ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?
            A: ì²­ì†Œ í›„ ë¬¸ì œ ë°œìƒ ì‹œ ë¬´ìƒ A/Së¥¼ ì œê³µí•©ë‹ˆë‹¤.
        """
    }

def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ì…ë‹ˆë‹¤.
    ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ ì„œë¹„ìŠ¤ ì í•©ë„ë¥¼ ì˜ˆì¸¡í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    try:
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        print("ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘...")
        model = torch.load("AI_RPA/unified_output/model.pt")
        tokenizer = AutoTokenizer.from_pretrained("klue/roberta-large")
        model.eval()
        
        # ì„œë¹„ìŠ¤ ì„¤ëª… ë¡œë“œ
        service_descriptions = load_service_descriptions()
        
        while True:
            # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
            print("\n=== ì„œë¹„ìŠ¤ ì í•©ë„ ë¶„ì„ ì‹œìŠ¤í…œ ===")
            print("ì¢…ë£Œí•˜ë ¤ë©´ 'q' ë˜ëŠ” 'quit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
            user_input = input("\nê³ ê°ë‹˜ì˜ ìš”êµ¬ì‚¬í•­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”: ")
            
            if user_input.lower() in ['q', 'quit']:
                print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            print("\në¶„ì„ ì¤‘...")
            results = []
            
            # ê° ì„œë¹„ìŠ¤ì— ëŒ€í•´ ì í•©ë„ ë¶„ì„
            for service_name, service_desc in service_descriptions.items():
                prediction = predict_suitability_improved(
                    user_input=user_input,
                    service_desc=service_desc,
                    model=model,
                    tokenizer=tokenizer
                )
                
                results.append({
                    "service_name": service_name,
                    "prediction": prediction
                })
            
            # ê²°ê³¼ ì •ë ¬ (ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ)
            results.sort(key=lambda x: x["prediction"]["confidence_score"], reverse=True)
            
            # ê²°ê³¼ ì¶œë ¥
            print("\n=== ë¶„ì„ ê²°ê³¼ ===")
            for result in results:
                service_name = result["service_name"]
                pred = result["prediction"]
                
                print(f"\nì„œë¹„ìŠ¤: {service_name}")
                print(f"ìµœì¢… ì ìˆ˜: {pred['confidence_score']:.2f}")
                
                if pred['confidence_score'] >= 0.7:
                    print("\nâœ… ì¶”ì²œ: ì´ ì„œë¹„ìŠ¤ëŠ” ê³ ê°ë‹˜ì˜ ìš”êµ¬ì‚¬í•­ê³¼ ë§¤ìš° ì˜ ë§ìŠµë‹ˆë‹¤.")
                elif pred['confidence_score'] >= 0.5:
                    print("\nğŸ¤” ì°¸ê³ : ì´ ì„œë¹„ìŠ¤ëŠ” ë¶€ë¶„ì ìœ¼ë¡œ ìš”êµ¬ì‚¬í•­ì„ ì¶©ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                else:
                    print("\nâŒ ì£¼ì˜: ì´ ì„œë¹„ìŠ¤ëŠ” ê³ ê°ë‹˜ì˜ ìš”êµ¬ì‚¬í•­ê³¼ ë§ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                
                print("\në¶„ì„ ë‚´ìš©:")
                for key, value in pred["explanation"].items():
                    if key != "qa_pairs":
                        print(f"- {value}")
            
            print("\në¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main() 