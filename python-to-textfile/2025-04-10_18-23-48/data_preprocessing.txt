import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer
import torch
from torch.utils.data import Dataset, DataLoader
import re
import os
import json
import random
from collections import Counter
from sklearn.utils import resample

class SoomgoDataset(Dataset):
    """
    텍스트 데이터를 처리하기 위한 PyTorch Dataset 클래스
    
    이 클래스는 파이토치의 Dataset을 상속받아 데이터셋을 관리합니다.
    """
    def __init__(self, request_texts, chat_texts, labels, service_types, tokenizer, max_length=512):
        """
        데이터셋 초기화
        
        Args:
            request_texts (list): 전처리된 요청서 텍스트 데이터 리스트
            chat_texts (list): 전처리된 채팅 텍스트 데이터 리스트
            labels (list): 라벨 데이터 리스트 (0: 적합, 1: 부적합)
            service_types (list): 서비스 유형 데이터 리스트
            tokenizer: BERT 토크나이저
            max_length (int): 최대 시퀀스 길이
        """
        self.request_texts = request_texts
        self.chat_texts = chat_texts
        self.labels = labels
        self.service_types = service_types
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __len__(self):
        """
        데이터셋의 총 샘플 수를 반환합니다.
        """
        return len(self.labels)
    
    def __getitem__(self, idx):
        """
        주어진 인덱스에 해당하는 샘플을 반환합니다.
        
        Args:
            idx (int): 샘플 인덱스
            
        Returns:
            dict: 토큰화된 입력 데이터와 라벨
        """
        request_text = str(self.request_texts[idx])
        chat_text = str(self.chat_texts[idx])
        label = self.labels[idx]
        service_type = self.service_types[idx]
        
        # 요청서와 채팅 텍스트 토큰화 (각각 따로)
        request_encoding = self.tokenizer(
            request_text,
            add_special_tokens=True,
            max_length=self.max_length // 2,  # 절반의 길이만 사용
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        chat_encoding = self.tokenizer(
            chat_text,
            add_special_tokens=True,
            max_length=self.max_length // 2,  # 절반의 길이만 사용
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'request_input_ids': request_encoding['input_ids'].flatten(),
            'request_attention_mask': request_encoding['attention_mask'].flatten(),
            'chat_input_ids': chat_encoding['input_ids'].flatten(),
            'chat_attention_mask': chat_encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long),
            'service_type': torch.tensor(service_type, dtype=torch.long)
        }

def load_data(json_path):
    """
    JSON 파일에서 데이터를 로드하는 함수
    
    Args:
        json_path (str): JSON 파일 경로
        
    Returns:
        pd.DataFrame: 로드된 데이터프레임
    """
    print(f"데이터 로드 중: {json_path}")
    
    # JSON 파일 경로가 공백을 포함하는 구 파일명을 참조하는 경우 새 파일명으로 대체
    if "soomgo data.json" in json_path:
        json_path = json_path.replace("soomgo data.json", "soomgo_data.json")
    
    # JSON 파일 읽기
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # 데이터를 리스트로 변환
    records = []
    for key, value in data.items():
        records.append(value)
    
    # 데이터프레임 생성
    df = pd.DataFrame(records)
    
    # 결측치 처리
    df['request_text'] = df['request_text'].fillna('')
    df['chat_text'] = df['chat_text'].fillna('')
    
    # '이사/입주청소업체' 서비스 카테고리만 선택
    df = df[df['service_category'].str.contains('이사/입주 청소업체')]
    
    # 데이터 통계 출력
    print(f"총 데이터 수: {len(df)}")
    print(f"적합한 요청 수: {sum(df['label'] == 0)}")
    print(f"부적합한 요청 수: {sum(df['label'] == 1)}")
    
    return df

def preprocess_request_text(text):
    """
    요청서 텍스트 전처리 함수
    
    Args:
        text (str): 전처리할 텍스트
        
    Returns:
        str: 전처리된 텍스트
    """
    # Q&A 구조에서 질문과 답변 모두 추출
    qa_pairs = []
    lines = text.split('\n')
    for i in range(len(lines)):
        line = lines[i]
        if '[Q]' in line and i + 1 < len(lines) and '[A]' in lines[i + 1]:
            question = line.split('[Q]')[1].strip()
            answer = lines[i + 1].split('[A]')[1].strip()
            if answer and answer != '고수와 상담시 논의할게요':
                qa_pairs.append(f"질문: {question} 답변: {answer}")
    
    # 질문-답변 쌍들을 하나의 텍스트로 결합
    processed_text = ' '.join(qa_pairs)
    
    # 특수문자 제거
    processed_text = re.sub(r'[^\w\s]', ' ', processed_text)
    # 연속된 공백 제거
    processed_text = re.sub(r'\s+', ' ', processed_text)
    # 앞뒤 공백 제거
    processed_text = processed_text.strip()
    
    return processed_text

def preprocess_chat_text(text):
    """
    채팅 텍스트 전처리 함수
    
    Args:
        text (str): 전처리할 텍스트
        
    Returns:
        str: 전처리된 텍스트
    """
    # 채팅 내용에서 유용한 정보 추출
    chat_lines = []
    for line in text.split('\n'):
        # 시간 정보와 같은 불필요한 부분 제거
        if ':' in line:
            parts = line.split(':', 1)
            if len(parts) == 2:
                content = parts[1].strip()
                if content and len(content) > 1:  # 짧은 응답 필터링
                    chat_lines.append(content)
    
    # 채팅 내용 결합
    processed_text = ' '.join(chat_lines)
    
    # 특수문자 제거
    processed_text = re.sub(r'[^\w\s]', ' ', processed_text)
    # 연속된 공백 제거
    processed_text = re.sub(r'\s+', ' ', processed_text)
    # 앞뒤 공백 제거
    processed_text = processed_text.strip()
    
    return processed_text

def extract_service_type(text):
    """
    Q&A 구조에서 첫 번째 답변을 기준으로 서비스 유형을 추출하는 함수
    
    Args:
        text (str): Q&A 형식의 텍스트
        
    Returns:
        int: 서비스 유형 인덱스 (0: 입주청소, 1: 이사청소, 2: 거주청소, 3: 기타)
    """
    # 첫 번째 질문과 답변 찾기
    lines = text.split('\n')
    for i, line in enumerate(lines):
        if '[Q] 어떤 서비스를 원하시나요?' in line and i + 1 < len(lines):
            answer_line = lines[i + 1]
            if '[A]' in answer_line:
                answer = answer_line.split('[A]')[1].strip()
                # 답변에서 키워드 포함 여부로 서비스 유형 판단
                if '입주청소' in answer:
                    return 0
                elif '이사청소' in answer:
                    return 1
                elif '거주청소' in answer:
                    return 2
                else:
                    return 3
    return 3  # 기본값: 기타

def augment_data(df, augmentation_factor=1.0):
    """
    데이터 증강 함수
    
    Args:
        df (pd.DataFrame): 원본 데이터프레임
        augmentation_factor (float): 증강 비율 (1.0이면 원본 데이터의 100% 추가)
        
    Returns:
        pd.DataFrame: 증강된 데이터프레임
    """
    # 부적합 데이터 (소수 클래스) 필터링
    minority_df = df[df['label'] == 1].copy()
    
    if len(minority_df) == 0:
        print("부적합 데이터가 없어 증강을 수행하지 않습니다.")
        return df
    
    # 증강할 샘플 수 계산
    num_to_augment = int(len(minority_df) * augmentation_factor)
    
    if num_to_augment == 0:
        return df
    
    augmented_samples = []
    
    for _ in range(num_to_augment):
        # 무작위로 샘플 선택
        sample = minority_df.sample(1).iloc[0]
        
        # 간단한 증강 기법 적용 (단어 순서 변경, 일부 단어 삭제 등)
        augmented_request = augment_text(sample['request_text'])
        augmented_chat = augment_text(sample['chat_text'])
        
        # 증강된 샘플 추가
        augmented_samples.append({
            'request_text': augmented_request,
            'chat_text': augmented_chat,
            'label': sample['label'],
            'service_category': sample['service_category'],
            'service_type': sample['service_type'] if 'service_type' in sample else extract_service_type(sample['request_text'])
        })
    
    # 원본 데이터프레임과 증강 데이터 결합
    augmented_df = pd.DataFrame(augmented_samples)
    combined_df = pd.concat([df, augmented_df], ignore_index=True)
    
    print(f"증강 전 데이터 수: {len(df)}")
    print(f"증강 후 데이터 수: {len(combined_df)}")
    print(f"증강 후 적합한 요청 수: {sum(combined_df['label'] == 0)}")
    print(f"증강 후 부적합한 요청 수: {sum(combined_df['label'] == 1)}")
    
    return combined_df

def augment_text(text):
    """
    텍스트 증강 함수
    
    Args:
        text (str): 원본 텍스트
        
    Returns:
        str: 증강된 텍스트
    """
    words = text.split()
    
    if len(words) <= 3:
        return text
    
    # 증강 기법 무작위 선택
    augmentation_type = random.choice(['swap', 'delete', 'synonym'])
    
    if augmentation_type == 'swap' and len(words) > 3:
        # 인접한 단어 위치 변경
        for _ in range(min(3, len(words) // 3)):
            idx = random.randint(0, len(words) - 2)
            words[idx], words[idx + 1] = words[idx + 1], words[idx]
    
    elif augmentation_type == 'delete':
        # 일부 단어 삭제 (최대 20%)
        num_to_delete = min(max(1, int(len(words) * 0.2)), 5)
        for _ in range(num_to_delete):
            if len(words) > 3:  # 최소 3개 단어는 유지
                del_idx = random.randint(0, len(words) - 1)
                words.pop(del_idx)
    
    elif augmentation_type == 'synonym':
        # 간단한 유의어 대체 시뮬레이션 (실제로는 더 정교한 유의어 사전이 필요)
        synonyms = {
            '청소': ['청소', '정리', '정돈', '클리닝'],
            '이사': ['이사', '이주', '거주이전', '이전'],
            '입주': ['입주', '새집', '신규입주', '신거주'],
            '거주': ['거주', '생활', '주거', '살이'],
            '필요': ['필요', '요구', '원함', '요청'],
            '원해요': ['원해요', '바랍니다', '희망합니다', '요청합니다'],
            '문의': ['문의', '질문', '요청', '상담'],
            '견적': ['견적', '가격', '비용', '요금']
        }
        
        for i, word in enumerate(words):
            if word in synonyms and random.random() < 0.3:  # 30% 확률로 유의어 대체
                words[i] = random.choice(synonyms[word])
    
    return ' '.join(words)

def prepare_data(df, tokenizer, test_size=0.2, val_size=0.1, random_state=42, augment=True, 
                batch_size=16, num_workers=4, pin_memory=True):
    """
    데이터 전처리 및 데이터셋 분할 함수
    
    Args:
        df (pd.DataFrame): 원본 데이터프레임
        tokenizer: BERT 토크나이저
        test_size (float): 테스트 세트 비율
        val_size (float): 검증 세트 비율
        random_state (int): 랜덤 시드
        augment (bool): 데이터 증강 여부
        batch_size (int): 배치 크기
        num_workers (int): 데이터 로딩에 사용할 워커 수
        pin_memory (bool): 메모리 고정 여부 (GPU 사용 시 True 권장)
        
    Returns:
        tuple: (train_loader, val_loader, test_loader)
    """
    # 텍스트 전처리 - 요청서와 채팅 별도 처리
    df['request_text_processed'] = df['request_text'].apply(preprocess_request_text)
    df['chat_text_processed'] = df['chat_text'].apply(preprocess_chat_text)
    
    # 서비스 유형 추출
    df['service_type'] = df['request_text'].apply(extract_service_type)
    
    # 데이터 클래스 불균형 확인
    label_counts = Counter(df['label'])
    print(f"레이블 분포: {label_counts}")
    
    # 데이터 증강 (부적합 데이터가 적은 경우)
    if augment and label_counts[1] < label_counts[0]:
        augmentation_factor = min(2.0, label_counts[0] / label_counts[1] - 1)  # 최대 2배까지 증강
        df = augment_data(df, augmentation_factor)
    
    # 데이터 분할
    train_val_df, test_df = train_test_split(
        df, test_size=test_size, random_state=random_state, stratify=df['label']
    )
    
    train_df, val_df = train_test_split(
        train_val_df, test_size=val_size/(1-test_size), 
        random_state=random_state, stratify=train_val_df['label']
    )
    
    # 데이터셋 생성
    train_dataset = SoomgoDataset(
        request_texts=train_df['request_text_processed'].values,
        chat_texts=train_df['chat_text_processed'].values,
        labels=train_df['label'].values,
        service_types=train_df['service_type'].values,
        tokenizer=tokenizer
    )
    
    val_dataset = SoomgoDataset(
        request_texts=val_df['request_text_processed'].values,
        chat_texts=val_df['chat_text_processed'].values,
        labels=val_df['label'].values,
        service_types=val_df['service_type'].values,
        tokenizer=tokenizer
    )
    
    test_dataset = SoomgoDataset(
        request_texts=test_df['request_text_processed'].values,
        chat_texts=test_df['chat_text_processed'].values,
        labels=test_df['label'].values,
        service_types=test_df['service_type'].values,
        tokenizer=tokenizer
    )
    
    # 클래스 가중치 계산 (학습용)
    class_weights = torch.tensor([
        1.0,
        label_counts[0] / label_counts[1] if label_counts[1] > 0 else 1.0
    ], dtype=torch.float)
    
    print(f"클래스 가중치: {class_weights}")
    
    # 데이터로더 생성 (GPU 최적화를 위한 설정 추가)
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True,
        num_workers=num_workers,  # 병렬 데이터 로딩
        pin_memory=pin_memory,    # GPU 메모리에 고정 (전송 속도 향상)
        prefetch_factor=2,       # 미리 로드할 배치 수
        persistent_workers=True if num_workers > 0 else False  # 워커 유지
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size * 2,  # 검증 시 더 큰 배치 사용 가능
        num_workers=num_workers,
        pin_memory=pin_memory,
        prefetch_factor=2,
        persistent_workers=True if num_workers > 0 else False
    )
    
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size * 2,  # 테스트 시 더 큰 배치 사용 가능
        num_workers=num_workers,
        pin_memory=pin_memory,
        prefetch_factor=2,
        persistent_workers=True if num_workers > 0 else False
    )
    
    return train_loader, val_loader, test_loader, class_weights

def save_preprocessed_data(df, output_dir):
    """
    전처리된 데이터를 저장하는 함수
    
    Args:
        df (pd.DataFrame): 전처리된 데이터프레임
        output_dir (str): 저장할 디렉토리 경로
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    output_path = os.path.join(output_dir, 'preprocessed_data.csv')
    df.to_csv(output_path, index=False)
    
    print(f"전처리된 데이터가 저장되었습니다: {output_path}")
    
    # 데이터 분포 시각화 및 저장
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    # 레이블 분포
    plt.figure(figsize=(12, 6))
    
    plt.subplot(1, 2, 1)
    sns.countplot(x='label', data=df, palette=['#3498db', '#e74c3c'])
    plt.title('레이블 분포')
    plt.xlabel('레이블 (0: 적합, 1: 부적합)')
    plt.ylabel('샘플 수')
    
    # 서비스 유형 분포
    plt.subplot(1, 2, 2)
    sns.countplot(x='service_type', data=df, palette='viridis')
    plt.title('서비스 유형 분포')
    plt.xlabel('서비스 유형 (0: 입주청소, 1: 이사청소, 2: 거주청소, 3: 기타)')
    plt.ylabel('샘플 수')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'data_distribution.png'))
    plt.close()
    
    print(f"데이터 분포 시각화가 저장되었습니다: {os.path.join(output_dir, 'data_distribution.png')}") 